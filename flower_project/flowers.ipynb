{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Flowers (Sunflower, Rose, Daisy, Tulip, Dandelion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = './train'\n",
    "test_dir = './test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data augmentation to better generalize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, \n",
    "                             zoom_range=0.02, \n",
    "                             rotation_range = 0.02)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(train_dir,\n",
    "                                            target_size=(150, 150), # all images will be resized to 150x150\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 250 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(test_dir,\n",
    "                                            target_size=(150, 150), # all images will be resized to 150x150\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_shape = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_125 (Conv2D)          (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 150, 150, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 75, 75, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              42468352  \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 43,282,725\n",
      "Trainable params: 43,282,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "import keras\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                                          histogram_freq=0, \n",
    "                                          batch_size=32, \n",
    "                                          write_graph=True, \n",
    "                                          write_grads=False, \n",
    "                                          write_images=True, \n",
    "                                          embeddings_freq=0, \n",
    "                                          embeddings_layer_names=None, \n",
    "                                          embeddings_metadata=None)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), data_format=\"channels_last\"))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), data_format=\"channels_last\")) \n",
    "\n",
    "model.add(Conv2D(128,(3,3), padding='same',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), data_format=\"channels_last\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(1024,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 113s 3s/step - loss: 1.6547 - acc: 0.2118 - val_loss: 1.6116 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 113s 3s/step - loss: 1.6044 - acc: 0.2328 - val_loss: 1.5783 - val_acc: 0.2440\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 1.4745 - acc: 0.3592 - val_loss: 1.4176 - val_acc: 0.3760\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 1.3548 - acc: 0.4474 - val_loss: 1.3213 - val_acc: 0.4400\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 1.2613 - acc: 0.4671 - val_loss: 1.4209 - val_acc: 0.4600\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 116s 3s/step - loss: 1.1530 - acc: 0.4974 - val_loss: 1.3860 - val_acc: 0.5320\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 113s 3s/step - loss: 1.1012 - acc: 0.5513 - val_loss: 1.2078 - val_acc: 0.4840\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.9503 - acc: 0.6249 - val_loss: 1.1958 - val_acc: 0.4760\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.7764 - acc: 0.6933 - val_loss: 1.5518 - val_acc: 0.5600\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.6928 - acc: 0.7302 - val_loss: 1.3734 - val_acc: 0.5200\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 0.5918 - acc: 0.7816 - val_loss: 1.4114 - val_acc: 0.5400\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 0.4221 - acc: 0.8290 - val_loss: 1.4948 - val_acc: 0.5720\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 0.5075 - acc: 0.8290 - val_loss: 1.7600 - val_acc: 0.4640\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 0.3597 - acc: 0.8776 - val_loss: 2.3104 - val_acc: 0.5360\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 114s 3s/step - loss: 0.1912 - acc: 0.9316 - val_loss: 2.3821 - val_acc: 0.4800\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.1735 - acc: 0.9448 - val_loss: 2.9489 - val_acc: 0.4920\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.2511 - acc: 0.9131 - val_loss: 2.3793 - val_acc: 0.4920\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.2080 - acc: 0.9263 - val_loss: 2.6282 - val_acc: 0.5280\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.1028 - acc: 0.9711 - val_loss: 3.2848 - val_acc: 0.4960\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.0327 - acc: 0.9882 - val_loss: 4.4394 - val_acc: 0.4720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c54d10e10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "        epochs=20,\n",
    "        validation_data=test_generator, \n",
    "        callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Augmentation (horizontal flip, rotation, zoom range) <br/>\n",
    "0.5760\n",
    "2. Remove horizontal flip <br/>\n",
    "0.5520\n",
    "3. Add an extra Dense layer with 512 nodes + extra dropout layer <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using Pre-Trained Model - Inception V3 then add custom final Dense layers (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(150, 150, 3)) \n",
    "inception_base = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = 750\n",
    "num_test = 250\n",
    "train_features = np.zeros(shape=(num_train, 3, 3, 2048))\n",
    "train_labels = np.zeros(shape=(num_train,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
